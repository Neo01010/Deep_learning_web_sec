IMDB数据集（Internet Movie Database）互联网电影资料库
特征提取方法：词袋模型、TF-IDF模型、词汇表模型、Word2Vec和Doc2vec模型

word2vec 是Google2013年开源的一款将词表征为实数值向量的高效工具，采用的模型有连续词袋（Continuous Bag-Of-Words, CBOW）模型和Skip-Gram两种。
Word2Vec通过训练，可以把对文本内容的处理简化为K维向量空间中的向量运算，而在向量空间上的相似度可以用来表示文本语义上的相似度。因此，word2vec输出
的词向量可以被用来做很多NLP相关的工作，如聚类、找同义词、词性分析等等。

CBOW模型能够根据周围n-1个词来预测出这个词的本身，而Skip-gram模型能够根据词本身来预测周围有哪些词。
CBOW模型的输入是某个词A周围的n个单词的词向量之和，输出是词A本身的词向量
Skip-gram模型的输入是词A的词向量，输出是词A周围的n个单词的词向量。

Word2Vec最常用的开源实现就是gensim


gensim使用非常简单，加载数据和训练数据可以合并，训练好模型后就可以按照单词获取对应的词向量表示

word2vec采用一个三层的神经网络。 训练的时候按照词频将每个词语Huffman编码，词频越高的词语对应的编码越短。
这三层的神经网络本身是对语言模型进行建模， 但同时获得一种单词在向量空间的表示。
与潜在语义分析（Latent Semantic Index, LSI）、潜在狄立克雷分配（Latent Dirichlet Allocation, LDA）的经典过程相比，
word2vec利用了词的上下文，语义信息更加丰富。


首先将高维one-hot表示的单词映射成低维向量，例如将10000维转换成300维，再在保留单词上下文的同时， 从一定程度上保留其意义。
可以通过skip-gram 和CBOW两种方法实现word2vec。 输入一个词，然后试着估计其他词出现在该词附近的概率，称为skip-gram方法。
与之相反的方法是CBOW， 通过给出一些上下文词语，然后通过评估概率找出最适合该上下文的词。
由于文本的长度各异，我们可能需要利用所有词向量的平均值作为分类算法的输入值，从而对整个文本文档进行分类处理。

然而，即使上述模型对词向量进行平均处理，我们仍然忽略了单词之间的排列顺序对情感分析的影响。
作为一个处理可变长度文本的总结性方法，Quoc Le 和 Tomas Mikolov 提出了 Doc2Vec方法。除了增加一个段落向量以外，这个方法几乎等同于 Word2Vec

word2vec只是基于词的维度进行“语义分析”的，并不具有上下文的“语义分析”能力。因此在word2vec的基础上增加一个段落向量，该方法是doc2vec。
该模型也有两个方法：Distributed Memory(DM) 和 Distributed Bag of Words(DBOW)。
DM试图给定上下文和段落向量的情况下预测单词的概率。在一个句子或者段落文档训练过程中，段落ID保存不变，共享同一个段落向量。
DBOW则在只给定段落向量的情况下预测段落中一组随机单词的概率。


doc2vec 的c-bow与word2vec的c-bow模型的区别

在训练过程中增加了每个句子的id(向量)，计算的时候将paragraph vector和word vector累加或者连接起来，作为softmax的输入。
在一个句子或者段落训练过程张，共享同一个paragraph vector，相当于在每次预测单词的概率时，都利用了整个句子的语义。
在预测过程，给预测句子分配一个新的paragraph id , 重新利用梯度下降训练待预测的句子，待收敛后，即得到待测句子的paragraph vector。
